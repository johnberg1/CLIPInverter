# CLIPInverter: CLIP-Guided StyleGAN Inversion for Text-driven Real Image Editing (ACM TOG 2023)

<a href="https://arxiv.org/abs/2307.08397"><img src="https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg"></a>

Inference Notebook: <a target="_blank" href="https://colab.research.google.com/github/johnberg1/CLIPInverter/blob/main/CLIPInverter_Inference.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

>Official Implementation of CLIP-Guided StyleGAN Inversion for Text-Driven Real Image Editing published in ACM TOG 2023 and presented in SIGGRAPH ASIA 2023 Syndey.


<p align="center">
<img src="assets/teaser_image_v3.jpg"/>  
<br>
We present CLIPInverter that enables users to easily perform semantic changes on images using free natural text. Our approach is not specific to a certain category of images and can be applied to many different domains (e.g., human faces, cats, birds) where a pretrained StyleGAN generator exists (top). Our approach specifically gives more accurate results for multi-attribute edits as compared to the prior work (middle). Moreover, as we utilize CLIPâ€™s semantic embedding space, it can also perform manipulations based on reference images without any training or finetuning (bottom).
</br>
</p>
